
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description" content="DreamMimic: Learning Visuomotor Whole-Body Loco-Manipulation via World Model">
    <meta name="keywords" content="humanoid loco-manipulation, visuomotor control, world model, policy distillation, RSSM, Dreamer, contact-rich interaction, OMOMO">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DreamMimic: Learning Visuomotor Whole-Body Loco-Manipulation via World Model</title>
      <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-L6PYDPEBZZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];

      function gtag() {
        dataLayer.push(arguments);
      }

      gtag('js', new Date());

      gtag('config', 'G-L6PYDPEBZZ');
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <style>
      /* Keep animated title effect with paper colors. */
      .dreammimic-title span {
        background-size: 420% 420%;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        animation: gradientShift 1.8s ease-in-out infinite, titleGlow 1.2s ease-in-out infinite;
      }

      .dreammimic-title .dream-word {
        background-image: linear-gradient(
          135deg,
          rgb(110, 40, 95) 0%,
          rgb(140, 60, 120) 45%,
          rgb(210, 130, 185) 100%
        );
      }

      .dreammimic-title .mimic-word {
        background-image: linear-gradient(
          135deg,
          rgb(35, 95, 135) 0%,
          rgb(54, 125, 189) 45%,
          rgb(130, 190, 240) 100%
        );
      }

      @keyframes titleGlow {
        0%, 100% {
          filter: brightness(1) saturate(1);
        }
        50% {
          filter: brightness(1.28) saturate(1.3);
        }
      }

      /* Video demo layout (keep consistent with Bulma/Nerfies look). */
      .demo-grid .box {
        height: 100%;
      }
      .demo-video {
        width: 100%;
        border-radius: 12px;
        background: #000;
        display: block;
      }
      .demo-caption {
        margin-top: 0.75rem;
        color: #4a4a4a;
        font-size: 0.95rem;
      }
      .demo-meta {
        margin-top: 0.35rem;
        color: #7a7a7a;
        font-size: 0.85rem;
      }
      /* Slightly narrower featured video (used for Sim2Sim). */
      .demo-video-narrow {
        max-width: 820px;
        margin-left: auto;
        margin-right: auto;
      }
    </style>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/video_comparison.js"></script>
  </head>
  <body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://DreamMimic.github.io">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
        </a>

        <!-- <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/SJTU-ViSYS/M2DGR">
              M2DGR(ICRA2022&RAL2021)
            </a>
            <a class="navbar-item" href="https://github.com/SJTU-ViSYS/Ground-Fusion">
              Ground-Fusion(ICRA2024)
            </a>
            <a class="navbar-item" href="https://github.com/sjtuyinjie/DAF">
              DAF(IROS2024)
            </a>
            <a class="navbar-item" href="https://github.com/Joanna-HE/LIGO.">
              LIGO(TRO2025&IROS2025)
            </a>
            <a class="navbar-item" href="https://delinqu.github.io/EN-SLAM/">
              EN-SLAM(CVPR2024)
            </a>
            <a class="navbar-item" href="https://link.springer.com/article/10.1007/s10291-024-01623-9">
              Innovation-KF(GPS Solutions2024)
            </a>
          </div>
        </div> -->
      </div>

    </div>
  </nav>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><span class="dreammimic-title"><span class="dream-word">Dream</span><span class="mimic-word">Mimic</span></span>: Learning Visuomotor Whole-Body Loco-Manipulation via World Model
              </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                <a href="" target="_blank">DreamMimic Authors</a>
                </span>
              </div>
              <div class="column is-full_width">
                <h2 class="is-size-6">(Author list and affiliations are anonymous.)</h2>
              <!-- <h2 class="is-size-6">(* equal contribution)  (<span>&#8224;</span> corresponding author)</h2> -->
            </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (TBD)</span>
                  </a>
                  </span>
                  <!--
                  <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark"
                    target="_blank">
                  <span class="icon">
                  <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (TBD)</span>
                  </a>
                  </span>
                  -->
                  <span class="link-block">
                  <a href="#Videos"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                  <i class="fas fa-video"></i>
                  </span>
                  <span>Video</span>
                  </a>
                  </span>
                  <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                  <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Will release upon acceptance)</span>
                  </a>
                  </span>
                  <!--
                  <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                  <i class="fas fa-cube"></i>
                  </span>
                  <span>Dataset (TBD)</span>
                  </a>
                  </span>
                  -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" style="margin-top: -20px">Overview</h2>
            <img src="./static/images/teaser.png" class="center" style="width:85%; height:auto;">
            <div class="content has-text-justified">
              <p style="margin-top: 30px">
                <!-- Description will be added here -->
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="notification has-text-weight-bold mb-6" 
               style="background: linear-gradient(135deg, #f5f9ff 0%, #e1edff 100%);
                      border-left: 4px solid #002D72; font-size: 0.92rem;">
            <div class="is-flex is-align-items-center">
              <span class="icon is-large mr-3" style="color: #0072CE;">
                <i class="fas fa-bolt"></i>
              </span>
              <div>
                <strong style="color: #002D72;">TL;DR:</strong> 
                <span style="display: inline; white-space: normal; color: #333;">
                  We present <strong>DreamMimic</strong>, a world-model-assisted framework that distills privileged teacher policies into <strong>vision-based humanoid whole-body loco-manipulation</strong>. 

                </span>
              </div>
            </div>
            </div>


            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
              Vision-based whole-body loco-manipulation on humanoid robots is challenging due to partial observability, contact-rich dynamics, and the difficulty of learning long-horizon behaviors from high-dimensional visual inputs. We present <strong style="color: #002D72;">DreamMimic</strong>, a framework that distills privileged teacher policies into vision-based humanoid controllers via world-model-assisted distillation. Instead of using a Dreamer-style RSSM for planning, we repurpose it to learn predictive latent dynamics that serve both as a representation space and a multi-step supervision signal, enabling latent alignment between teacher and student and substantially reducing long-horizon drift.
              Beyond standard reconstruction objectives for proprioceptive and visual observations, we incorporate a <em>Reward Prediction Head</em> (RPH) that predicts the instantaneous task reward to anchor the latent state to task-relevant signals. Additionally, we propose <em>Interaction-Aware Prediction Heads</em> (IAPH) for contact and object state modeling. By supervising interaction cues that are difficult to infer from vision alone, IAPH promotes a latent space better aligned with the demands of contact-rich loco-manipulation, where accurate contact and object dynamics prediction is critical for long-horizon success.
              We further introduce <em>Performance-Conditioned Guidance (PCG)</em>, a reward-driven adaptive distillation schedule that computes performance scores for both teacher and student to dynamically balance guidance and exploration. PCG prevents both premature teacher annealing and excessive teacher interference in challenging visual settings.
              Experiments on OMOMO and BEHAVE dataset demonstrate improved manipulation performance, robust vision-based control without privileged information, and consistent cross-simulator transfer across humanoid platforms. Our results highlight world models as an effective mechanism for stabilizing visual policy distillation in complex humanoid behaviors. We will release the code to benefit the research community upon paper acceptance.
            </p>

            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" style="margin-top: -30px">Contributions</h2>
          
            <div class="content has-text-justified">
              <p style="margin-top: 10px">
                <b>Vision-based whole-body loco-manipulation.</b> DreamMimic learns to reproduce contact-rich humanoid-object interactions from vision and proprioception, mitigating long-horizon drift with multi-step latent supervision from a predictive world model.
              </p>
            </div>
            <div class="content has-text-justified">
              <p style="margin-top: 30px">
                <b>Interaction-aware predictive representations.</b> Reward- and interaction-aware auxiliary heads (RPH + IAPH) shape the latent space to better capture task-relevant signals such as contact transitions and object dynamics under partial observability. 
              </p>
            </div>
            <div class="content has-text-justified">
              <p style="margin-top: 30px">
                <b>Stable training via reward-conditioned teacher mixing.</b> A reward-conditioned teacher-mixing schedule adapts the amount of teacher guidance during DAgger+RL, preventing premature annealing in challenging visual settings. (Figure will be updated.)
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" style="margin-top: -20px">Pipeline</h2>
            <img src="./static/images/pipeline.png" class="center" style="width:60%; height:auto;">
            <div class="content has-text-justified">
              <p style="margin-top: 30px">
                <!-- Description will be added here -->
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" style="margin-top: -30px">World Model</h2>
            <img src="./static/images/world_model.png" class="center" style="width:80%; height:auto;">
            <div class="content has-text-justified">
              <p style="margin-top: 10px">
                <!-- Description will be added here -->
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section" id="Videos">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" style="margin-top: -30px">Video Demos</h2>
            <div class="content has-text-justified">
              <p style="margin-top: 10px">
                The videos below present qualitative results of DreamMimic on <strong>long-horizon</strong> and <strong>contact-rich</strong> whole-body loco-manipulation. Overall, we use <strong>world-model-assisted distillation</strong> to transfer privileged teacher skills (e.g., interaction/object cues and rich visual signals such as depth/segmentation) into a student policy that operates from vision and proprioception, and we evaluate robustness across datasets, simulators, and platforms.
              </p>
            </div>

            <div class="columns is-multiline demo-grid">
              <div class="column is-half">
                <div class="box has-text-left">
                  <div class="title is-5 mb-2">SMPL-X Carrying a Box (Depth + Segmentation Inputs Shown)</div>
                  <video class="demo-video" autoplay loop muted playsinline preload="metadata">
                    <source src="./static/videos/box.mp4" type="video/mp4">
                  </video>
                  <div class="demo-caption">
                    <strong>Box (SMPL-X)</strong>: the left panel visualizes depth and semantic segmentation inputs, while the right panel shows the resulting interaction behavior. This task stresses long-horizon modeling of object dynamics and contact transitions under partial observability.
                  </div>
                </div>
              </div>

              <div class="column is-half">
                <div class="box has-text-left">
                  <div class="title is-5 mb-2">SMPL-X on OMOMO: Carrying a Chair</div>
                  <video class="demo-video" autoplay loop muted playsinline preload="metadata">
                    <source src="./static/videos/chair.mp4" type="video/mp4">
                  </video>
                  <div class="demo-caption">
                    <strong>Chair (OMOMO, SMPL-X)</strong>: on the OMOMO human-object interaction benchmark, the policy must coordinate locomotion and manipulation, maintain stable hand-object contact, and regulate whole-body balance while transporting the chair.
                  </div>
                </div>
              </div>

              <div class="column is-half">
                <div class="box has-text-left">
                  <div class="title is-5 mb-2">SMPL-X on BEHAVE: Interacting with a Container</div>
                  <video class="demo-video" autoplay loop muted playsinline preload="metadata">
                    <source src="./static/videos/container.mp4" type="video/mp4">
                  </video>
                  <div class="demo-caption">
                    <strong>Container (BEHAVE, SMPL-X)</strong>: in BEHAVE interaction scenes, the character performs contact-rich manipulation with a container. This setting highlights the need for interaction-aware state prediction and long-horizon stability in the learned representations.
                  </div>
                </div>
              </div>

              <div class="column is-half">
                <div class="box has-text-left">
                  <div class="title is-5 mb-2">G1 Carrying a Box (Embodiment-Flexible)</div>
                  <video class="demo-video" autoplay loop muted playsinline preload="metadata">
                    <source src="./static/videos/g1box.mp4" type="video/mp4">
                  </video>
                  <div class="demo-caption">
                    <strong>G1-Box</strong>: we transfer the learned interaction skill to the G1 platform for box carrying, demonstrating cross-platform generalization and stable control under load and contact constraints (gait-manipulation coupling).
                  </div>
                </div>
              </div>

              <div class="column is-full">
                <div class="box has-text-left">
                  <div class="title is-5 mb-2">Sim2Sim: SMPL-X Chair Lifting in Isaac Sim</div>
                  <video class="demo-video demo-video-narrow" autoplay loop muted playsinline preload="metadata">
                    <source src="./static/videos/isaaclab.mp4" type="video/mp4">
                  </video>
                  <div class="demo-caption">
                    <strong>Sim2Sim</strong>: we demonstrate sim-to-sim transfer in Isaac Sim for chair lifting, validating that world-model-assisted distillation preserves behavior quality and interaction stability across physics engines and rendering conditions.
                  </div>
                  <div class="demo-meta">
                    
                  </div>
                </div>
              </div>
            </div>

          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{dreammimic2026,
  title     = {DreamMimic: Learning Visuomotor Whole-Body Loco-Manipulation via World Model},
  author    = {Anonymous Authors},
  booktitle = {TBD},
  year      = {2026}
}</code></pre>
      </div>
    </section>
  <!-- <section class="section" id="RelatedWorks">
    <div class="container is-max-desktop content">
      <h2 class="title">Related Works</h2>
      <pre><code>@article{hafner2019dreamer,
  title   = {Dream to Control: Learning Behaviors by Latent Imagination},
  author  = {Hafner, Danijar and others},
  journal = {arXiv},
  year    = {2019}
}

@article{xu2025intermimic,
  title   = {InterMimic: Scalable Teacher--Student Distillation for Humanoid--Object Interaction},
  author  = {Xu, TBD and others},
  journal = {arXiv},
  year    = {2025}
}</code></pre>
    </div>
  </section> -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
              <p>
                This website template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
                We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
